{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27afb730",
   "metadata": {},
   "source": [
    "# üìò Day 2 ‚Äì LLMs as Semantic Instruments\n",
    "\n",
    "## Why Meaning Exploration Matters\n",
    "\n",
    "Understanding **meaning** in text‚Äîwhether through q**ualitative interpretation** or **quantitative analysis**‚Äîis critical for research across disciplines. It allows to uncover patterns, compare perspectives, and derive insights from unstructured data. While traditional methods rely on manual coding or keyword searches, AI-driven approaches (like generative models and sentence embeddings) enable scalable, nuanced, and reproducible analysis.\n",
    "\n",
    "**Use Cases & Examples**\n",
    "\n",
    "- Compare theoretical frameworks in literature reviews.\n",
    "- Analyze political discourse for shifts in rhetoric over time.\n",
    "- Cluster survey responses to identify emerging themes.\n",
    "- Track customer sentiment in product reviews.\n",
    "- Detect semantic similarities in legal documents for case law research.\n",
    "- Automate thematic tagging in large-scale reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Current Approaches to Meaning Exploration\n",
    "\n",
    "| **Approach**                                                 | **Use**                                                                    | **Limitations**                                            | **Emerging Alternatives**                                                                                             |\n",
    "| ------------------------------------------------------------ | -------------------------------------------------------------------------- | ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Generative AI**<br>(e.g., Gemini, GPT-4)                   | - Qualitative interpretation<br>- Summarization<br>- Hypothesis generation | - Hallucinations<br>- API dependency<br>- Cost             | - Smaller, domain-specific LLMs (e.g., BioBERT)<br>- Open-weight models (Llama 3, Mistral)                            |\n",
    "| **Sentence Embeddings**<br>(e.g., SBERT, all-MiniLM-L6-v2)   | - Semantic similarity<br>- Clustering<br>- Search                          | - Struggles with irony/sarcasm<br>- Static representations | - Dynamic embeddings (e.g., `text-embedding-3`)<br>- Multimodal embeddings (text + image/audio)                       |\n",
    "| **Hybrid**<br>(Embeddings + Generative AI)                   | - Scalable analysis<br>- Human-readable insights                           | - Integration complexity                                   | - RAG (Retrieval-Augmented Generation)<br>- Auto-labeling (e.g., Prodigy + Active Learning)                           |\n",
    "| **Traditional Methods**<br>(Manual Coding, Lexical Analysis) | - Ground-truth validation<br>- Small datasets                              | - Labor-intensive<br>- Subjective                          | - AI-assisted coding (e.g., NVivo + LLMs)<br>- Rule-based with LLM fallbacks                                          |\n",
    "| **Experimental**                                             | - Neurosymbolic AI<br>- Attention Mechanisms<br>- Knowledge Graphs         | *N/A (frontier work)*                                      | - Combining symbolic logic with LLMs<br>- BERT-style attention visualization<br>- Entity linking via knowledge graphs |\n",
    "\n",
    "**How to Choose an Approach**:\n",
    "\n",
    "| **Need**              | **Best Current Tool**  | **Future Alternative**       |\n",
    "| --------------------- | ---------------------- | ---------------------------- |\n",
    "| Explain nuances       | Generative AI          | Neurosymbolic AI             |\n",
    "| Cluster 10k documents | Sentence Embeddings    | Multimodal Embeddings        |\n",
    "| Validate hypotheses   | Hybrid + Manual Coding | AI-assisted annotation tools |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcc66d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Gemini Meaning Probes\n",
    "\n",
    "This approach uses language understanding to generate human-like explanations of meaning.\n",
    "\n",
    "Best for:\n",
    "\n",
    "- Qualitative, nuanced comparisons (e.g., \"How do these sentences differ in intent?\")\n",
    "\n",
    "- Contextual understanding (e.g., political implications, tone, implied meaning)\n",
    "\n",
    "- When you need a narrative explanation rather than raw data\n",
    "\n",
    "\n",
    "\n",
    "Use Google Gemini (via API) to interpret sentence meaning.\n",
    "\n",
    "‚û°Ô∏è Requires a free [Gemini API key](https://makersuite.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69223c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Retrieve the API key from Secrets\n",
    "api_key = userdata.get('GEMINI_API_KEY')  # Name must match what you set!\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b2a0ae",
   "metadata": {},
   "source": [
    "Compare Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72deb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "prompt = \"Compare the meaning of: 'The minister supported the bill.' and 'The minister opposed the bill.'\"\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1fb1a",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Hugging Face Sentence Embeddings\n",
    "\n",
    "We now compute vector representations using `sentence-transformers`.\n",
    "\n",
    "This approach converts text into numerical vectors (embeddings) that capture semantic meaning.\n",
    "\n",
    "It allows mathematical comparison (e.g., **cosine similarity**) of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a712eb",
   "metadata": {},
   "source": [
    "**Which model to use**\n",
    "\n",
    "The [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model is a popular default for sentence embeddings due to its balance of speed and performance, but it may not always be the best fit. \n",
    "\n",
    " **Why `all-MiniLM-L6-v2`?**\n",
    ">\n",
    "> - It is **small and fast**, ideal for live demos or classroom settings.\n",
    "> - Trained specifically for **semantic similarity tasks**, making it highly effective for comparing sentence meanings.\n",
    "> - Outputs **384-dimensional vectors**, balancing speed with representational depth.\n",
    "> - Part of the `sentence-transformers` library, maintained by Hugging Face and SBERT.net.\n",
    "> - Pretrained on **general and question-answer datasets**, making it robust for diverse domains.\n",
    "\n",
    "\n",
    "Not the best choice if:\n",
    "\n",
    "- You need multilingual support (it‚Äôs English-only).\n",
    "\n",
    "- Your task requires domain-specific understanding (e.g., legal, biomedical).\n",
    "\n",
    "- You need higher-dimensional embeddings for fine-grained similarity.\n",
    "\n",
    "**Sentence Embedding Models ‚Äì General English (Better Performance)**\n",
    "\n",
    "| **Model**                   | **Dimensions** | **Speed** | **Use Case**                                           |\n",
    "| --------------------------- | -------------- | --------- | ------------------------------------------------------ |\n",
    "| `all-mpnet-base-v2`         | 768            | Slower    | Higher accuracy than MiniLM. Best for semantic search. |\n",
    "| `gte-base` (*General Text*) | 768            | Medium    | Stronger out-of-the-box performance than MiniLM.       |\n",
    "| `e5-base-v2` (*Embeddings*) | 768            | Medium    | Optimized for retrieval tasks (e.g., RAG).             |\n",
    "\n",
    "\n",
    "**üèõÔ∏è Sentence Embedding Models ‚Äì Domain-Specific Tasks**\n",
    "\n",
    "| **Model**                  | **Domain** | **Dimensions** | **Notes**                                       |\n",
    "| -------------------------- | ---------- | -------------- | ----------------------------------------------- |\n",
    "| `BioBERT` *(Hugging Face)* | Biomedical | 768            | Fine-tuned for medical/life sciences.           |\n",
    "| `Legal-BERT`               | Legal      | 768            | Trained on court cases and contracts.           |\n",
    "| `FinBERT`                  | Financial  | 768            | Optimized for earnings reports and SEC filings. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "    \"The minister supported the bill.\",\n",
    "    \"The minister opposed the bill.\",\n",
    "    \"The bill was popular among voters.\",\n",
    "    \"Many citizens disagreed with the proposal.\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(\"Embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b54cfd",
   "metadata": {},
   "source": [
    "**How to Test Alternatives?**\n",
    "\n",
    "Compare Embedding Quality: \n",
    "Lower values indicate better discrimination between opposites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238101bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "models = {\n",
    "    \"MiniLM\": \"all-MiniLM-L6-v2\",\n",
    "    \"MPNet\": \"all-mpnet-base-v2\",\n",
    "    \"BGE\": \"BAAI/bge-large-en-v1.5\"\n",
    "}\n",
    "\n",
    "sentences = [\"The minister supported the bill.\", \"The minister opposed the bill.\"]\n",
    "\n",
    "for name, model_name in models.items():\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(sentences)\n",
    "    sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    print(f\"{name}: Similarity = {sim:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b016da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dd8c938",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Meaning Matrix Heatmap\n",
    "\n",
    "We compute cosine similarity between sentence embeddings and plot a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(sim_matrix, xticklabels=sentences, yticklabels=sentences, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Cosine Similarity Between Sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eec513",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Semantic Drift Demo\n",
    "\n",
    "Example showing how small changes in wording can change meaning vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36743854",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_sentences = [\n",
    "    \"The protest was peaceful.\",\n",
    "    \"The riot turned violent.\",\n",
    "    \"The demonstration gathered thousands.\",\n",
    "    \"The violent clash disrupted the city.\"\n",
    "]\n",
    "\n",
    "drift_embeddings = model.encode(drift_sentences)\n",
    "drift_sim = cosine_similarity(drift_embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(drift_sim, xticklabels=drift_sentences, yticklabels=drift_sentences, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"Semantic Drift ‚Äì Framing Differences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41b8d9",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Annotator Disagreement Simulation \n",
    "\n",
    "Simulate two annotators assigning sentiment to the same set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c175e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coder_A = [\n",
    "    \"The project was successful.\",\n",
    "    \"The project had issues.\",\n",
    "    \"The plan worked well.\",\n",
    "    \"The initiative was flawed.\"\n",
    "]\n",
    "\n",
    "coder_B = [\n",
    "    \"The plan was a success.\",\n",
    "    \"There were serious flaws.\",\n",
    "    \"The outcome was positive.\",\n",
    "    \"The result was problematic.\"\n",
    "]\n",
    "\n",
    "emb_A = model.encode(coder_A)\n",
    "emb_B = model.encode(coder_B)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "combined = np.vstack([emb_A, emb_B])\n",
    "labels = ['A']*4 + ['B']*4\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(combined)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for i, label in enumerate(labels):\n",
    "    plt.scatter(reduced[i, 0], reduced[i, 1], label=f\"{label} {i%4 + 1}\")\n",
    "plt.title(\"Annotator Meaning Space\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa3495d",
   "metadata": {},
   "source": [
    "## üîÅ Recap & What‚Äôs Next\n",
    "\n",
    "- Gemini shows qualitative interpretation\n",
    "- HF gives numerical vectors\n",
    "- Cosine and PCA reveal deep semantic structure\n",
    "\n",
    "‚û°Ô∏è In Session 2, we‚Äôll use these vectors for classification, clustering, and retrieval.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969ea80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

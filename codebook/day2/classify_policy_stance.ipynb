{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7f182b",
   "metadata": {},
   "source": [
    "# Classify Policy Stance with Embeddings\n",
    "\n",
    "In this notebook, we demonstrate how to classify text (e.g. survey responses or policy statements) by using sentence embeddings. We'll start with a zero-shot setup using cosine similarity, and gradually introduce more advanced techniques including few-shot classification and shallow supervised models. Finally, we outline how parameter-efficient fine-tuning (PEFT) can further improve accuracy for domain-specific tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ba335",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b505b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e02957",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification with Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ba613",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_examples = {\n",
    "    \"Supportive\": [\n",
    "        \"The policy is necessary to reduce emissions\",\n",
    "        \"I believe climate change is a priority\"\n",
    "    ],\n",
    "    \"Neutral\": [\n",
    "        \"I don’t have strong views on the matter\",\n",
    "        \"This needs more information\"\n",
    "    ],\n",
    "    \"Opposed\": [\n",
    "        \"I think it's a waste of taxpayer money\",\n",
    "        \"We should not enforce such rules\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af47b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_embedding(text):\n",
    "    return model.encode([text])[0]\n",
    "\n",
    "def classify_text(text, anchors):\n",
    "    text_vec = get_embedding(text)\n",
    "    scores = {}\n",
    "    for label, examples in anchors.items():\n",
    "        sims = [cosine_similarity([text_vec], [get_embedding(ex)])[0][0] for ex in examples]\n",
    "        scores[label] = np.mean(sims)\n",
    "    return max(scores, key=scores.get), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = [\n",
    "    \"This policy will help the environment\",\n",
    "    \"Why are we spending on this?\",\n",
    "    \"I haven’t read enough to say\"\n",
    "]\n",
    "\n",
    "for text in unlabeled:\n",
    "    label, scores = classify_text(text, anchor_examples)\n",
    "    print(f\"Text: '{text}'\\n → Predicted: {label}\\n → Scores: {scores}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d25b81",
   "metadata": {},
   "source": [
    "## Few-Shot Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a041d9a",
   "metadata": {},
   "source": [
    "Add more anchor examples per label category to improve semantic coverage. More examples = more accurate average vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28861b18",
   "metadata": {},
   "source": [
    "## Embedding + Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127aba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training data\n",
    "all_texts = [\n",
    "    \"The policy will reduce CO2\", \"We must act on climate\", \"Neutral position\", \n",
    "    \"Not enough info\", \"This policy wastes money\", \"Completely disagree with this\"\n",
    "]\n",
    "all_labels = [\"Supportive\", \"Supportive\", \"Neutral\", \"Neutral\", \"Opposed\", \"Opposed\"]\n",
    "\n",
    "# Get sentence embeddings\n",
    "X = [get_embedding(text) for text in all_texts]\n",
    "y = all_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942e799",
   "metadata": {},
   "source": [
    "## Optional: Fine-Tuning with LoRA\n",
    "\n",
    "For advanced users or domain-specific tasks, you can improve model performance by fine-tuning the embedding model using parameter-efficient fine-tuning (PEFT) techniques such as:\n",
    "\n",
    "- LoRA (Low-Rank Adaptation)\n",
    "- Adapters\n",
    "- QLoRA (Quantized LoRA for low-resource training)\n",
    "\n",
    "See GitBook page [`peft_finetune_demo.md`](peft_finetune_demo.md) for examples and links.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71892ba3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

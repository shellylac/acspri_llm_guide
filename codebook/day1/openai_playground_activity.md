---
id: huggingface_chat_activity
title: Prompt Exploration Activity (Hugging Face in Colab)
description: Hands-on prompt testing activity using Hugging Face pipelines in Google Colab.
---



# ğŸ§ª Prompting Activity: Exploring Prompts in OpenAI Playground

> Explore how prompt structure and model settings impact GPT-4 or GPT-3.5 output directly inside the OpenAI Playground.

---

## ğŸ”¹ 1. Access the Playground

1. Go to [https://platform.openai.com/playground](https://platform.openai.com/playground)
2. Log in with your OpenAI account
3. Set your model to **GPT-4** or **GPT-3.5** (if GPT-4 access is enabled)

---

## ğŸ”¹ 2. Copy This Prompt Template

Paste into the text area:

```
You are a science communicator. Rewrite the abstract below for a Year 10 student audience. Use plain language. Maximum 100 words.

## Abstract:
[Paste abstract here]
```

âœ… Tip: Use the **"Insert"** button to test with a variety of pre-written abstracts or content snippets

---

## ğŸ”¹ 3. Adjust Model Settings (Right Panel)

| Parameter         | Description                              | Suggested Range                     |
| ----------------- | ---------------------------------------- | ----------------------------------- |
| Temperature       | Controls randomness/creativity           | 0.2â€“0.4 = precise0.7â€“1.0 = creative |
| Maximum length    | Maximum number of tokens in response     | 100â€“300 for short tasks             |
| Top P             | Probabilistic sampling diversity control | Leave default                       |
| Frequency Penalty | Reduces repetition                       | Use 0.2â€“0.5 for summaries           |
| Stop Sequences    | Define where the model should stop       | Optional                            |

ğŸ§  *Start with Temperature 0.3 for factual prompts, 0.9 for brainstorming.*

---

## ğŸ”¹ 4. Try These Prompt Variants

| Type       | Modification                               |
| ---------- | ------------------------------------------ |
| Persona    | â€œExplain like a TED speaker.â€              |
| Format     | â€œReturn as 3 bullet points.â€               |
| Style      | â€œUse short sentences and avoid jargon.â€    |
| Constraint | â€œInclude a real-world example or analogy.â€ |

---

## ğŸ”¹ 5. Evaluate Output

Use this mini-rubric:

| Criterion    | GPT Output Score (1â€“5) |
| ------------ | ---------------------- |
| Clarity      | â˜ 1 â˜ 2 â˜ 3 â˜ 4 â˜ 5    |
| Relevance    | â˜ 1 â˜ 2 â˜ 3 â˜ 4 â˜ 5    |
| Format Match | â˜ 1 â˜ 2 â˜ 3 â˜ 4 â˜ 5    |

âœï¸ Optional: Copy your best output and try it again with a **slightly revised prompt**. What changes?

---

## ğŸ”¹ 6. (Optional) Compare With Gemini or Claude

Try the same prompt in:

- [Gemini Pro](https://aistudio.google.com)
- [Claude](https://claude.ai)

Reflect:

- Does GPT follow structure better?
- Which model aligned more with your audience?
- How does Temperature affect tone across tools?

---

## âœ… Summary

Youâ€™ve now practiced:

- Designing precise vs. creative prompts
- Adjusting OpenAI model behavior using Temperature, Max Length, and Penalties
- Evaluating model outputs for clarity and alignment

This forms the basis for structured prompt testing inside research, teaching, and prototype workflows.


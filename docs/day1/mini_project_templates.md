---

---

# ğŸ§ª Mini Project Templates (Prompting)

> â€œThe best way to master prompting is to *apply* it on a problem you understand.â€

This page offers plug-and-play **prompt templates** that participants can use to:

- Apply prompting frameworks to real-world tasks
- Adapt templates to their own research or sector
- Practice on their own after the live session

Each template includes:

- ğŸ§  Task
- ğŸ§± Prompt structure
- ğŸ› ï¸ Example (in plain text)
- ğŸ” Variant paths to try (Zero-shot, Few-shot, CoT, Structured)
- ğŸ”¬ Technical or research implementation pattern

---

## ğŸ§  Research Hypothesis Generator

**Task:** Generate testable research hypotheses based on literature or qualitative insights.

**Prompt Template:**

```yaml
system: "You are a PhD-level research assistant."
task: "Generate 3 research hypotheses based on the following excerpt."
format: Bullet list, each < 30 words
input: [Paste interview summary, paper excerpt, or report finding]
```

**Example Output:**

- Users with chronic illness are more likely to use online forums for emotional support.
- Privacy concerns reduce likelihood of AI adoption in regional clinics.
- Teacher burnout correlates with perceived policy instability.

**Variants:**

- Add method suggestion: â€œSuggest method to test each hypothesisâ€
- Switch from bullet to structured table (theme | hypothesis | method)
- Use this to build surveys or protocols

**Technical Path:**

- Chain to a JSON-to-CSV converter for survey builders
- Build prompt registry under `prompts/research/hypothesis_builder.md`
- Use in CoT format for literature synthesis-based generation

---

## ğŸ“Š Research Summary Comparator

**Task:** Compare two article abstracts or study summaries across dimensions (e.g. theory, data, region).

**Prompt Template:**

```yaml
system: "You are a comparative research analyst."
task: "Compare these two abstracts across topic, method, region, and findings."
format: Markdown table
input: [Abstract A] [Abstract B]
```

**Example Output:**

| Category | Abstract A                  | Abstract B                        |
| -------- | --------------------------- | --------------------------------- |
| Topic    | climate risk                | adaptation planning               |
| Method   | interview                   | panel regression                  |
| Region   | Australia                   | ASEAN                             |
| Findings | social trust moderates risk | migration patterns shape response |

**Variants:**

- Include rating for strength of method
- Extend into automatic synthesis: â€œWhat is the policy implication of both combined?â€

**Technical Notes:**

- Prompt lives in `prompts/research/compare_articles.yaml`
- Use in paired dataset workflow: compare hundreds of abstracts programmatically

---

## ğŸ’¼ Business Strategy Evaluator

**Task:** Evaluate a business strategy case (e.g. startup plan, public document, press release) across viability, innovation, and risk.

**Prompt Template:**

```yaml
system: "You are a strategic analyst evaluating a business plan."
task: "Score this strategy across 3 dimensions: viability, innovation, execution risk."
format: JSON
input: [Paste strategy, executive summary, or pitch deck text]
```

**Example Output:**

```json
{
  "viability": [4, "Target market is well-defined, pricing model tested"],
  "innovation": [3, "Moderately novel, but competitors exist"],
  "execution_risk": [2, "Team has prior experience and capital"]
}
```

**Variants:**

- Use multiple roles: analyst, investor, CTO
- Add CoT rationale: â€œExplain each score before providingâ€
- Use slider interface in Streamlit to manually adjust scores

**Research Application:**

- Compare business models in case studies
- Automate pre-screening of MBA venture projects
- Link prompt output to rubric/learning outcome system

**Technical Pattern:**

- Store prompt as `prompts/business/strategy_eval.md`
- Call via Streamlit + plot radar chart
- Export JSON to Airtable or Notion

---

ğŸ“ [Download Starter Notebook â†’](link) ğŸ“ [See Prompt Anatomy â†’](prompt_anatomy.md) ğŸ“ [See Prompt Frameworks â†’](prompt_frameworks.md) ğŸ“ [Try Prompt Evaluation â†’](prompt_evaluation.md)

